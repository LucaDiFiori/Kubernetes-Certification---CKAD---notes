
Per capire cos'è l'Ingress in Kubernetes, partiamo da un esempio pratico.

Immagina di gestire un negozio online, disponibile su un dominio come **my-online-store.com**. Hai creato un'applicazione con un'immagine Docker e l'hai distribuita come **Pod** in un **Deployment** nel tuo cluster Kubernetes. Questa applicazione utilizza un database MySQL, che hai configurato come un altro Pod con un servizio **ClusterIP**, chiamato `mysql-service`, per renderlo accessibile solo all'interno del cluster.

Ora il tuo negozio funziona, ma per renderlo accessibile agli utenti esterni hai bisogno di un servizio di tipo **NodePort**. Questo servizio assegna una porta alta, ad esempio **38080**, che gli utenti possono utilizzare per accedere al negozio tramite un URL come `http://<IP-del-nodo>:38080`.

![[Pasted image 20241217102006.png]]

#### Problemi delle configurazioni base

Man mano che il traffico cresce e aggiungi più repliche dei Pod per bilanciare il carico, il servizio NodePort continua a svolgere il suo lavoro, distribuendo il traffico tra i Pod. Tuttavia, sorgono alcuni problemi:

1. **[[DNS (Domain Name System)]]**: Non vuoi che gli utenti ricordino l'IP del nodo, quindi configuri un server DNS per puntare al tuo dominio. Gli utenti possono accedere al sito tramite `http://my-online-store.com:38080`.
2. **Porte**: Vuoi eliminare l'uso delle porte nel URL, ma NodePort assegna solo porte alte (>30000). Per risolvere, puoi configurare un proxy che reindirizzi il traffico dalla porta 80 alla porta 38080.
   
   ![[Pasted image 20241217102202.png]]


Se il tuo negozio fosse ospitato su un **cloud pubblico**, come Google Cloud Platform, potresti semplificare la configurazione utilizzando un servizio di tipo **LoadBalancer** (invece che NodePort), che crea automaticamente un bilanciatore di carico nativo con un IP pubblico. Basta puntare il DNS a questo IP e il gioco è fatto.

![[Pasted image 20241217102404.png]]


#### Espandere l'applicazione: nuovi problemi

Con la crescita del business, aggiungi un nuovo servizio, come un'applicazione per lo streaming video, accessibile su `my-online-store.com/watch`. La vecchia applicazione rimane su `my-online-store.com/wear`.

Dato che lo streaming è un'applicazione separata, la distribuisci come un nuovo Deployment con un servizio dedicato (`video-service`). Ma ogni nuovo servizio di tipo LoadBalancer crea un nuovo bilanciatore di carico, aumentando i costi e la complessità. Ora hai bisogno di:

1. Un sistema che gestisca il traffico basato sugli URL (es. `/wear` o `/watch`).
2. Una configurazione centralizzata per SSL, bilanciamento del carico e regole.

![[Pasted image 20241217102745.png]]



***
## Soluzione: Ingress in Kubernetes

L'**Ingress** risolve questi problemi. 

In poche parole L'Ingress consente agli utenti di accedere all'applicazione tramite un unico [[URL]] pubblico, che può essere configurato per instradare il traffico a diversi servizi all'interno del cluster in base al percorso o al dominio dell'URL. Inoltre, semplifica la gestione della sicurezza implementando il supporto per SSL/TLS, garantendo connessioni sicure.

Con Ingress puoi:

- Gestire tutto il traffico tramite un unico URL pubblico.
- Instradare il traffico a diversi servizi in base a:
	- **path** o 
	- al **dominio**.
- Terminate SSL direttamente nell'Ingress.

**Nota importante**: Per esporre l'Ingress all'esterno del cluster, devi comunque utilizzare un NodePort o un LoadBalancer, ma questa configurazione è necessaria solo una volta.

![[Pasted image 20241217105227.png]]


Con Ingress puoi semplificare la gestione del traffico, ridurre i costi (eliminando bilanciatori di carico multipli) e centralizzare le configurazioni di sicurezza e bilanciamento direttamente in Kubernetes.




***
## Come funziona Ingress

Per implementare l'Ingress, hai bisogno di due elementi principali:

1. **Ingress Controller**: È il motore che gestisce le richieste. Non è incluso di default in Kubernetes, quindi devi installarne uno (es. **NGINX**, Traefik o HAProxy).
   
2. **Ingress Resource**: È una risorsa Kubernetes che definisce le regole per il routing del traffico (es. quale URL punta a quale servizio).






***
***
# CONFIGURARE UN INGRESS:

## 1. DISTRIBUIRE L'INGRESS CONTROLLER

![[Pasted image 20241217105546.png]]

Per utilizzare un Ingress all'interno di un cluster Kubernetes, è necessario prima distribuire un componente chiamato **Ingress Controller**. Questo elemento è responsabile di interpretare e applicare le regole definite nelle risorse Ingress, fungendo da intermediario tra il traffico esterno e i servizi interni del cluster.

L'Ingress Controller non è incluso di default in Kubernetes, quindi bisogna sceglierne uno e installarlo. Esistono diverse opzioni disponibili, come **NGINX**, **Traefik**, o soluzioni specifiche offerte dai cloud provider (ad esempio, il GKE Ingress per Google Cloud). 

Una volta scelto l'Ingress Controller, viene distribuito come un **Deployment** o DaemonSet nel cluster, configurando i relativi servizi e risorse necessari. Ad esempio, se si sceglie il controller NGINX, l'installazione può essere semplificata utilizzando `kubectl` con un file YAML predefinito o strumenti come Helm per gestire l'intero processo.

## Deployment definition file per un Ingress Controller Nginx

Per **configurare un Ingress Controller NGINX** su Kubernetes, dobbiamo seguire una serie di passaggi fondamentali. L'obiettivo è rendere NGINX operativo nel cluster e pronto a gestire il traffico in ingresso. Vediamo come farlo.

#### a) Creare il Deployment per il l' Ingress Controller
Il **Deployment** è il cuore della configurazione, perché definisce come Kubernetes deve avviare e gestire il **pod** che esegue NGINX.

- Utilizziamo un'immagine specifica di NGINX ottimizzata per lavorare come **Ingress Controller** all'interno di Kubernetes.
  
- NGINX, in questa immagine, è avviato da un comando specifico situato in `/nginx-ingress-controller`. Quindi, dobbiamo indicarlo esplicitamente come **argomento** nel deployment.
  
```yaml
(...)
args:
	- /nginx-ingress-controller
```

- Impostiamo un singolo **replica**, ma potremmo aumentarlo in base alle esigenze di traffico.


#### b) Separare le Configurazioni con ConfigMap
NGINX richiede spesso configurazioni personalizzate come:

- Percorsi per i file di **log**,
- Parametri per la gestione delle connessioni,
- Impostazioni SSL.

Per non "ingessare" queste configurazioni all'interno del pod, utilizziamo una **ConfigMap**. Questo approccio rende la gestione più semplice e ci permette di aggiornare le configurazioni senza toccare direttamente il Deployment.

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-configuration
data:
  error-log-path: "/var/log/nginx/error.log"
  keep-alive: "true"
  ssl-protocols: "TLSv1.2 TLSv1.3"
```

Per specificare poi quale ConfigMap deve essere utilizzata per configurare il comportamento del container NGINX, passiamo in `args` (argomenti di configurazione con cui il processo principale del container deve essere avviato) questo elemento:
```yaml
(...)
args:
	- /nginx-ingress-controller
	- --configmap=$(POD_NAMESPACE)/nginx-configuration
```



#### c) Impostare Variabili d'Ambiente
All'interno del pod, NGINX ha bisogno di conoscere due informazioni fondamentali:

- Il **nome del pod** in cui è in esecuzione,
- Il **namespace** del cluster in cui è distribuito.

Questi dati vengono recuperati dinamicamente tramite **variabili d'ambiente** (`POD_NAME` e `POD_NAMESPACE`). Kubernetes ci permette di ottenerli automaticamente utilizzando i **fieldRef**.

#### d) Configurare le Porte per il Traffico
Infine, dobbiamo specificare le **porte** su cui NGINX riceverà le richieste:

- La **porta 80** per il traffico HTTP,
- La **porta 443** per il traffico HTTPS.

Queste porte sono quelle standard e permetteranno all'Ingress Controller di gestire le connessioni provenienti dall'esterno del cluster.


```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-ingress-controller
spec:
  replicas: 1
  selector:
    matchLabels:
      name: nginx-ingress
  template:
    metadata:
      labels:
        name: nginx-ingress
    spec:
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
          args:
            - /nginx-ingress-controller
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 80
            - name: https
              containerPort: 443
```

Con questo file YAML, abbiamo definito:

1. **Il Deployment**: Kubernetes avvia NGINX con tutte le impostazioni necessarie.
2. **La ConfigMap**: le configurazioni di NGINX sono separate, facili da modificare e gestire.
3. **Le variabili d'ambiente**: dinamicamente recuperate da Kubernetes per adattarsi all'ambiente di esecuzione.
4. **Le porte**: che permettono a NGINX di ascoltare il traffico in ingresso.

Ora NGINX è pronto per funzionare come Ingress Controller nel nostro cluster Kubernetes.



### Esporre l'ingress controller con un NodePort
Ora abbiamo bisogno di un servizio per esporre l'Ingress controller al modno esterno.
Creiamo quindi un service di ripo **NodePort**, con il selector che corrisponda all'etichetta `name: nginx-ingress` impostata sul deployment 

![[Pasted image 20241217120608.png]]



### Fornire un ServiceAccount all' Ingress Controller
Come accennato in precedenza, gli **Ingress Controller** offrono funzionalità avanzate che consentono di monitorare il cluster alla ricerca di risorse **Ingress** e di configurare dinamicamente il server NGINX sottostante ogni volta che vengono rilevate delle modifiche.

Per poter svolgere questa attività, l'Ingress Controller deve avere i **permessi necessari** per interagire con il cluster Kubernetes. Questi permessi vengono garantiti attraverso un **Service Account**, che fornisce al controller l'accesso ai **ruoli** e alle **risorse** del cluster.

In pratica, il Service Account permette all'Ingress Controller di:

1. **Accedere** alle risorse di tipo Ingress presenti nel cluster.
2. **Monitorare** eventuali modifiche a queste risorse.
3. **Configurare** automaticamente il server NGINX per adattarsi ai cambiamenti rilevati.

In assenza di un Service Account adeguato, l'Ingress Controller non avrebbe i privilegi necessari per eseguire queste operazioni fondamentali.

![[Pasted image 20241217134817.png]]



#### Per riassumere
Un **Ingress Controller** richiede i seguenti componenti per funzionare correttamente:

- **Deployment**: utilizza un'immagine NGINX specifica per agire come Ingress Controller.
- **Service**: serve a esporre il controller all'interno del cluster o verso l'esterno.
- **ConfigMap**: fornisce i dati di configurazione necessari per personalizzare il comportamento di NGINX, come percorsi dei log o impostazioni SSL.
- **Service Account**: garantisce i permessi adeguati per monitorare e configurare le risorse Ingress nel cluster.


Dopo l'installazione, l'Ingress Controller monitora costantemente le risorse Ingress definite nel cluster e applica le regole di instradamento richieste, configurando automaticamente il bilanciamento del carico e la gestione delle connessioni sicure, se necessario.





***
## 2. CREARE UN INGRESS RESOURCE
Una volta distribuito l'Ingress Controller, il passaggio successivo è **creare una risorsa Ingress** (Ingress Resource). Questa risorsa è un oggetto Kubernetes che definisce un insieme di regole e configurazioni, per permettere all'ingress controller, di instradamento del traffico esterno verso i servizi interni del cluster.

Esempio:
Considerando l'esempio dell'inizio, un ingress resoruce può specificare che, a seconda dell'URL inserito, il traffico venga indirizzara ad un applicazione o l'altra

`www.my-online-store.com/wear` --> instrada al portale di vendita vestiti
`www.my-online-store.com/watch` --> instrada al portale di video


### Creare un Ingress Resource
Per creare un Ingress Resource, si utilizza un file YAML in cui si specificano dettagli come i percorsi degli URL, i servizi target e, se necessario, la configurazione SSL per connessioni sicure. 

Ad esempio, puoi definire che le richieste indirizzate a `www.esempio.com/api` vengano inoltrate al servizio interno chiamato `backend-api`, mentre quelle per `www.esempio.com/web` vadano a `frontend-web`.

Il traffico verrà indirizzato ai services dell'app e non direttamente ai pods, la sezione
**`--> spec.backend`**
definisce dove verrà instradato il traffico. 
se c'è un singolo back-end non c'è bisogno di rules, basta specificare serviceName e port

Un esempio di configurazione YAML per un Ingress Resource potrebbe essere:

![[Pasted image 20241217141638.png]]

Dove:
- La sezione **`spec.backend`** la **destione dove sarà indirizzato traffico**
- 80 : porta del service `wear-service` che espone il backend

A questo punto l'ingress viene creato e tutto il traffico è instradato al `wear-service`


### sezione: Rules
La sezione **`rules`** in un file YAML di un oggetto **Ingress** serve a definire le regole che controllano **come il traffico HTTP o HTTPS viene instradato verso i servizi interni del cluster Kubernetes**. È qui che si specificano i dettagli del traffico, come il nome del dominio (opzionale), i percorsi (URL) e i servizi backend associati.

Possiamo configurare delle **regole** per instradare il traffico in base a determinate condizioni, come:
- il **dominio** o 
- il **nome host** utilizzato dagli utenti.


Ad esempio:
- Possiamo creare una **regola** per gestire il traffico proveniente da un **dominio** specifico. Se gli utenti accedono al cluster utilizzando il dominio `my-online-store.com`, il traffico può essere instradato seguendo la **regola 1**.
- Allo stesso modo, se gli utenti accedono utilizzando un sottodominio come `wear.my-online-store.com`, possiamo creare una **regola separata** (ad esempio, la **regola 2**) per instradare e gestire questo traffico in modo distinto.

In questo modo, è possibile separare il traffico in base al dominio o sottodominio e configurare comportamenti personalizzati per ciascuna regola.

![[Pasted image 20241217144805.png]]


All'interno di ogni **regola** è possibile configurare percorsi specifici per instradare il traffico verso servizi diversi.

Ad esempio, all'interno della **regola 1**, possiamo gestire:
- il percorso `/wear` per instradare il traffico verso l'applicazione dedicata all'abbigliamento,
- il percorso `/watch` per indirizzare il traffico verso l'applicazione video,
- un percorso predefinito per tutto il traffico restante, garantendo così una gestione chiara e organizzata delle diverse applicazioni.

![[Pasted image 20241217145302.png]]


Stessa cosa la posso fare per le altre **rules**. La rule 2 potrà gestire tutti i percorsi verso applicazione collegare al `www.wear. ...`

![[Pasted image 20241217145508.png]]

In questo caso, digitando `www.wear.my-online-store.com`, l'utente verrà indirizzato alla pagina principale dello shop. Se invece si digita `wear.my-online-store.com/support`, il traffico verrà instradato verso backend specifici dedicati al supporto, permettendo così di gestire diverse funzionalità in base al percorso dell'URL.



Quindi:
![[Pasted image 20241217145800.png]]

- Possiamo avere **rules** per ogni host o nome dominio
- All'interno di ogni regola possiamo avere percorsi per instradare il traffico in base all'URL



# Configurare l'ingress resorce
Quando si parla di routing del traffico in un sistema come Kubernetes, è possibile farlo utilizzando sia l'[[URL ]](Uniform Resource Locator) che il [[hostname]]. Ecco una spiegazione di entrambi i concetti e delle loro differenze.

## --> Instradare il traffico tramite URL
Il nostro obiettivo è gestire tutto il traffico in arrivo verso il nostro store online, instradandolo in base al percorso dell'URL. Per questo motivo, abbiamo bisogno di una sola regola, poiché stiamo gestendo il traffico per un unico dominio, `my-online-store.com`, che ha però diversi percorsi


![[Pasted image 20241217150148.png]]

Il **routing del traffico per URL** implica che il sistema instradi le richieste in base al **percorso** specificato nell'URL. Questo significa che puoi definire regole di routing che instradano il traffico in base a diverse parti del percorso. Ad esempio:

- Tutte le richieste che iniziano con `/api` potrebbero essere indirizzate a un servizio API.
- Tutte le richieste che iniziano con `/static` potrebbero essere indirizzate a un servizio che gestisce file statici.

Questo tipo di routing è utile quando si desidera separare diverse funzionalità o risorse della stessa applicazione, utilizzando un unico hostname.


All'interno delle regole, troviamo un elemento chiamato `-http[]`, che ci consente di specificare diversi percorsi. Questo significa che i percorsi sono organizzati in un array, con ciascun elemento rappresentante un percorso specifico per un determinato URL.

![[Pasted image 20241217151255.png]]
##### Spiegazione 

**spec**: Questa sezione definisce la configurazione dell'oggetto Ingress.
    - **rules**: Qui si definiscono le regole per il routing del traffico.
        - **http**: Indica che le regole seguono il protocollo HTTP.
            - **paths**: Questa sezione contiene le diverse regole di routing basate sui percorsi.
                - **path: /wear**: Specifica che il traffico in ingresso su `/wear` deve essere indirizzato a un servizio specifico.
                    - **backend**: Definisce il servizio di backend a cui deve essere indirizzato il traffico.
                        - **service**:
                            - **name: wear-service**: Nome del servizio a cui inviare il traffico per questo percorso.
                            - **servicePort: 80**: Porta del servizio a cui inviare il traffico (porta HTTP standard).
                - **path: /watch**: Specifica che il traffico in ingresso su `/watch` deve essere indirizzato a un altro servizio.
                    - **backend**: Definisce il servizio di backend a cui deve essere indirizzato il traffico.
                        - **service**:
                            - **name: watch-service**: Nome del servizio a cui inviare il traffico per questo percorso.
                            - **servicePort: 80**: Porta del servizio a cui inviare il traffico.






## --> Instradare il traffico in base all'hostname - Sezione: Host
Il **routing del traffico per hostname** implica che il sistema instradi le richieste in base al **nome di dominio** specificato nell'URL. Ciò significa che puoi avere regole di routing che separano il traffico in base ai diversi nomi di dominio. Ad esempio:

- Tutto il traffico per `api.example.com` potrebbe essere instradato a un servizio API.
- Tutto il traffico per `www.example.com` potrebbe essere instradato a un servizio web principale.

Questo tipo di routing è utile per gestire più applicazioni o microservizi che condividono lo stesso cluster, consentendo l'uso di nomi di dominio distinti per ciascuna applicazione.

La sezione **host** in un file YAML di un oggetto Ingress in Kubernetes è utilizzata per specificare il **nome di dominio (o hostname) a cui si applicano le regole di routing definite nell'Ingress** stesso. Questa sezione consente di instradare il traffico verso servizi diversi in base al nome di dominio utilizzato nella richiesta HTTP.

Ora che ho due domini creerò due **rules**:
![[Pasted image 20241217155758.png]]

Quando si creano due **rules** in un oggetto Ingress per gestire il traffico in base a diversi domini, si utilizza il campo **host** per specificare il nome di dominio associato a ciascuna regola. Questo campo deve corrispondere al valore del nome di dominio utilizzato nell'URL della richiesta. Grazie a questa configurazione, il traffico verrà instradato verso il backend appropriato in base al dominio richiesto.

** --> NOTA**: Se **non si specifica il campo host**, la regola verrà considerata come un jolly e accetterà tutto il traffico, cioè da tutti gli host 
(`qualsiasi_nome_host_basta_che_finisca_con\quellochespecificoqui`). 
Questo significa che la regola si applicherà a tutte le richieste, senza limitazioni basate sul dominio utilizzato.

Questa formulazione chiarisce che l'assenza del campo **host** comporta l'accettazione di tutte le richieste e sottolinea che le regole si applicano senza restrizioni basate sul nome del dominio. 



#### Struttura della sezione host
Quando definisci un oggetto Ingress, la sezione host si inserisce all'interno della sezione **rules**. Ecco un esempio di come può apparire:

![[Pasted image 20241217160455.png]]


La sezione host in un file YAML di un Ingress è fondamentale per il routing del traffico HTTP in Kubernetes. Consente di gestire in modo flessibile e sicuro le richieste basate sul nome di dominio, migliorando la scalabilità e la manutenibilità delle applicazioni nel cluster.



## Differenze Chiave routing per URL e hostname

- **Granularità**:
    - **URL**: Permette di suddividere il traffico in base al percorso, consentendo una maggiore granularità all'interno dello stesso dominio.
    - **Hostname**: Divide il traffico a livello di dominio, permettendo di gestire servizi distinti sotto nomi di dominio separati.
      
- **Scenari di Utilizzo**:
    - **URL**: Utile quando si desidera organizzare le funzionalità di un'applicazione all'interno di un dominio comune.
    - **Hostname**: Utile per applicazioni completamente separate o per implementazioni di microservizi in ambienti multitenant.




***
***
# Aggiornamenti Ingress nella versione Kubernetes 1.20+

### **Principali Modifiche**
1. **apiVersion**:
    
    - **Prima (obsoleta):** `extensions/v1beta1`
        - Questa versione dell'API era in stato **deprecato** e non supportata a partire da **Kubernetes 1.22**.
    - **Ora (attuale):** `networking.k8s.io/v1`
        - La nuova versione è **stabile** e dovrebbe essere utilizzata per definire gli Ingress.


***

### Backend:
- **Prima (obsoleta)** 
```yaml
backend:
  serviceName: wear-service
  servicePort: 80
```
Qui, il backend specificava i campi `serviceName` e `servicePort` direttamente.

- **attuale**
```yaml
backend:
  service:
    name: wear-service
    port:
      number: 80
```

Il backend ora utilizza una nuova struttura nidificata sotto `service` con:

- **`name`**: Nome del servizio Kubernetes.
- **`port.number`**: Numero della porta del servizio.


***

### PathType
Nella versione **`networking.k8s.io/v1`**, è obbligatorio specificare il campo **`pathType`**
Il campo **`pathType`** specifica come l'Ingress Controller deve interpretare il valore del percorso definito (`path`). Kubernetes offre tre opzioni: `Exact`, `Prefix` e `ImplementationSpecific`. Ecco la spiegazione dettagliata con esempi pratici.

- **-->`Exact`**: Il percorso deve corrispondere esattamente.
	- Il percorso specificato nella regola deve **corrispondere esattamente** all'URL della richiesta.
	- **Non viene considerato alcun prefisso o estensione** dell'URL oltre al valore fornito.

Esempio:
```yaml
- path: /wear
  pathType: Exact
  backend:
    service:
      name: wear-service
      port:
        number: 80
```
- La richiesta a `http://example.com/wear` viene inviata al servizio **`wear-service`**.
- La richiesta a `http://example.com/wear/extra` o `http://example.com/wear/` viene ignorata perché **non corrisponde esattamente**.




- **-->`Prefix`**: Il percorso è trattato come prefisso.
	- Il percorso viene interpretato come un **prefisso**.
	- Qualsiasi URL che **inizia** con il percorso specificato corrisponderà alla regola.

Esempio:
```yaml
- path: /wear
  pathType: Prefix
  backend:
    service:
      name: wear-service
      port:
        number: 80

```

-  La richiesta a `http://example.com/wear` corrisponde.
- La richiesta a `http://example.com/wear/shoes` corrisponde perché **inizia** con `/wear`.
- La richiesta a `http://example.com/watch` **non** corrisponde perché non ha il prefisso `/wear`.

🔹 **Nota:** Questo è il comportamento più comunemente utilizzato e supportato da tutti gli Ingress Controller.





- **-->`ImplementationSpecific`**: 
	-  Il comportamento del percorso dipende dall'**implementazione specifica** dell'Ingress Controller utilizzato (es. NGINX, Traefik, HAProxy).
	- Ogni Ingress Controller può definire come gestire i percorsi, il che significa che il comportamento **non è standardizzato**.


**Esempio**:
```yaml
- path: /wear
  pathType: ImplementationSpecific
  backend:
    service:
      name: wear-service
      port:
        number: 80
```

- Il comportamento del percorso dipende dal controller:
    - In NGINX, potrebbe comportarsi come `Prefix` per default.
    - In un altro controller, potrebbe comportarsi diversamente.
- **Devi controllare la documentazione** del controller per sapere come verrà interpretato il percorso.



***

### **Riepilogo delle Differenze**

|**Campo**|**Versione Obsoleta (v1beta1)**|**Versione Aggiornata (v1)**|
|---|---|---|
|**API Version**|`extensions/v1beta1`|`networking.k8s.io/v1`|
|**Service Backend**|`serviceName` e `servicePort`|`service.name` e `service.port.number`|
|**PathType**|Non presente|Obbligatorio (`Prefix`, `Exact`, `ImplementationSpecific`)|


### Esempio Aggiornato Completo
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
spec:
  rules:
    - http:
        paths:
        - path: /wear
          pathType: Prefix
          backend:
            service:
              name: wear-service
              port:
                number: 80
        - path: /watch
          pathType: Prefix
          backend:
            service:
              name: watch-service
              port:
                number: 80
```









***
***
## COMANDI 

**Creare un Ingress Resource**
``` bash
kubectl create -f ingress.yaml
```

**Visualizzare gli ingress Resorces (non il deployment ingress contoll)**
```bash
kubectl get ingress
```

**Ottenere dettagli su un Ingress specifico**
```bash
kubectl describe ingress <nome-ingress>
```
##### Informazioni Comuni nell'Output
1. **Name**: Il nome dell'oggetto Ingress.
    
2. **Namespace**: Il namespace in cui si trova l'Ingress.
    
3. **Annotations**: Eventuali annotazioni associate all'Ingress, che possono fornire informazioni aggiuntive o configurazioni specifiche per l'Ingress Controller.
    
4. **Class**: La classe dell'Ingress, se è stata specificata. Questo determina quale Ingress Controller gestisce questa risorsa.
    
5. **Rules**: Le regole di routing definite per l'Ingress, inclusi i percorsi e i servizi backend associati:
    - **Host**: Il nome dell'host per cui è valida la regola (se specificato).
    - **Path**: I percorsi definiti (come `/wear` e `/watch`).
    - **Service**: Il nome del servizio di backend e la porta a cui inviare il traffico.

6. **Address**: L'indirizzo IP o il nome del DNS dell'Ingress, se è stato esposto e registrato correttamente.
    
7. **Events**: Eventuali eventi associati all'Ingress, che possono indicare stati di errore o altre attività recenti relative all'Ingress.



#### **IMPERATIVE: Creare Ingress resource**
```bash
kubectl create ingress <ingress-name> --rule="host/path=service:port"
```
Crea un oggetto Ingress con una regola specificata.

Esempio:
```bash
kubectl create ingress <ingress-name> --rule="wear.my-online-store.com/wear*=wear-service:80"
```
Instrada il traffico per **`wear.my-online-store.com`** con percorso `/wear*` verso il servizio **`wear-service`** sulla porta **80**.



#### **Creare un ingress in un namespace**
**1. Vedere quale service è presente nel namespace**
```bash
k get svc -n <nome-namespace>
```

Da qui andrò a vedere:
- `NAME`: Il nome del service (es: pay-service)
- `PORT`: La porta su cui è esposto il servizio (es: 8282)

**2. Imperative command**
```bash
k create ingress <nome-ingress> -n <nome-namespace> --rule="/pay=<nome-servizio>:<porta-del-servizio"
```
- `/pay` : tuttp il traffico indirizzato a /pay
- `=` : andrà
- `<nome-servizio>`: a questo servizio
- `<porta-servizio>`: su questa porta