
Per capire cos'√® l'Ingress in Kubernetes, partiamo da un esempio pratico.

Immagina di gestire un negozio online, disponibile su un dominio come **my-online-store.com**. Hai creato un'applicazione con un'immagine Docker e l'hai distribuita come **Pod** in un **Deployment** nel tuo cluster Kubernetes. Questa applicazione utilizza un database MySQL, che hai configurato come un altro Pod con un servizio **ClusterIP**, chiamato `mysql-service`, per renderlo accessibile solo all'interno del cluster.

Ora il tuo negozio funziona, ma per renderlo accessibile agli utenti esterni hai bisogno di un servizio di tipo **NodePort**. Questo servizio assegna una porta alta, ad esempio **38080**, che gli utenti possono utilizzare per accedere al negozio tramite un URL come `http://<IP-del-nodo>:38080`.

![[Pasted image 20241217102006.png]]

#### Problemi delle configurazioni base

Man mano che il traffico cresce e aggiungi pi√π repliche dei Pod per bilanciare il carico, il servizio NodePort continua a svolgere il suo lavoro, distribuendo il traffico tra i Pod. Tuttavia, sorgono alcuni problemi:

1. **[[DNS (Domain Name System)]]**: Non vuoi che gli utenti ricordino l'IP del nodo, quindi configuri un server DNS per puntare al tuo dominio. Gli utenti possono accedere al sito tramite `http://my-online-store.com:38080`.
2. **Porte**: Vuoi eliminare l'uso delle porte nel URL, ma NodePort assegna solo porte alte (>30000). Per risolvere, puoi configurare un proxy che reindirizzi il traffico dalla porta 80 alla porta 38080.
   
   ![[Pasted image 20241217102202.png]]


Se il tuo negozio fosse ospitato su un **cloud pubblico**, come Google Cloud Platform, potresti semplificare la configurazione utilizzando un servizio di tipo **LoadBalancer** (invece che NodePort), che crea automaticamente un bilanciatore di carico nativo con un IP pubblico. Basta puntare il DNS a questo IP e il gioco √® fatto.

![[Pasted image 20241217102404.png]]


#### Espandere l'applicazione: nuovi problemi

Con la crescita del business, aggiungi un nuovo servizio, come un'applicazione per lo streaming video, accessibile su `my-online-store.com/watch`. La vecchia applicazione rimane su `my-online-store.com/wear`.

Dato che lo streaming √® un'applicazione separata, la distribuisci come un nuovo Deployment con un servizio dedicato (`video-service`). Ma ogni nuovo servizio di tipo LoadBalancer crea un nuovo bilanciatore di carico, aumentando i costi e la complessit√†. Ora hai bisogno di:

1. Un sistema che gestisca il traffico basato sugli URL (es. `/wear` o `/watch`).
2. Una configurazione centralizzata per SSL, bilanciamento del carico e regole.

![[Pasted image 20241217102745.png]]



***
## Soluzione: Ingress in Kubernetes

L'**Ingress** risolve questi problemi. 

In poche parole L'Ingress consente agli utenti di accedere all'applicazione tramite un unico [[URL]] pubblico, che pu√≤ essere configurato per instradare il traffico a diversi servizi all'interno del cluster in base al percorso o al dominio dell'URL. Inoltre, semplifica la gestione della sicurezza implementando il supporto per SSL/TLS, garantendo connessioni sicure.

Con Ingress puoi:

- Gestire tutto il traffico tramite un unico URL pubblico.
- Instradare il traffico a diversi servizi in base a:
	- **path** o 
	- al **dominio**.
- Terminate SSL direttamente nell'Ingress.

**Nota importante**: Per esporre l'Ingress all'esterno del cluster, devi comunque utilizzare un NodePort o un LoadBalancer, ma questa configurazione √® necessaria solo una volta.

![[Pasted image 20241217105227.png]]


Con Ingress puoi semplificare la gestione del traffico, ridurre i costi (eliminando bilanciatori di carico multipli) e centralizzare le configurazioni di sicurezza e bilanciamento direttamente in Kubernetes.




***
## Come funziona Ingress

Per implementare l'Ingress, hai bisogno di due elementi principali:

1. **Ingress Controller**: √à il motore che gestisce le richieste. Non √® incluso di default in Kubernetes, quindi devi installarne uno (es. **NGINX**, Traefik o HAProxy).
   
2. **Ingress Resource**: √à una risorsa Kubernetes che definisce le regole per il routing del traffico (es. quale URL punta a quale servizio).






***
***
# CONFIGURARE UN INGRESS:

## 1. DISTRIBUIRE L'INGRESS CONTROLLER

![[Pasted image 20241217105546.png]]

Per utilizzare un Ingress all'interno di un cluster Kubernetes, √® necessario prima distribuire un componente chiamato **Ingress Controller**. Questo elemento √® responsabile di interpretare e applicare le regole definite nelle risorse Ingress, fungendo da intermediario tra il traffico esterno e i servizi interni del cluster.

L'Ingress Controller non √® incluso di default in Kubernetes, quindi bisogna sceglierne uno e installarlo. Esistono diverse opzioni disponibili, come **NGINX**, **Traefik**, o soluzioni specifiche offerte dai cloud provider (ad esempio, il GKE Ingress per Google Cloud). 

Una volta scelto l'Ingress Controller, viene distribuito come un **Deployment** o DaemonSet nel cluster, configurando i relativi servizi e risorse necessari. Ad esempio, se si sceglie il controller NGINX, l'installazione pu√≤ essere semplificata utilizzando `kubectl` con un file YAML predefinito o strumenti come Helm per gestire l'intero processo.

## Deployment definition file per un Ingress Controller Nginx

Per **configurare un Ingress Controller NGINX** su Kubernetes, dobbiamo seguire una serie di passaggi fondamentali. L'obiettivo √® rendere NGINX operativo nel cluster e pronto a gestire il traffico in ingresso. Vediamo come farlo.

#### a) Creare il Deployment per il l' Ingress Controller
Il **Deployment** √® il cuore della configurazione, perch√© definisce come Kubernetes deve avviare e gestire il **pod** che esegue NGINX.

- Utilizziamo un'immagine specifica di NGINX ottimizzata per lavorare come **Ingress Controller** all'interno di Kubernetes.
  
- NGINX, in questa immagine, √® avviato da un comando specifico situato in `/nginx-ingress-controller`. Quindi, dobbiamo indicarlo esplicitamente come **argomento** nel deployment.
  
```yaml
(...)
args:
	- /nginx-ingress-controller
```

- Impostiamo un singolo **replica**, ma potremmo aumentarlo in base alle esigenze di traffico.


#### b) Separare le Configurazioni con ConfigMap
NGINX richiede spesso configurazioni personalizzate come:

- Percorsi per i file di **log**,
- Parametri per la gestione delle connessioni,
- Impostazioni SSL.

Per non "ingessare" queste configurazioni all'interno del pod, utilizziamo una **ConfigMap**. Questo approccio rende la gestione pi√π semplice e ci permette di aggiornare le configurazioni senza toccare direttamente il Deployment.

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-configuration
data:
  error-log-path: "/var/log/nginx/error.log"
  keep-alive: "true"
  ssl-protocols: "TLSv1.2 TLSv1.3"
```

Per specificare poi quale ConfigMap deve essere utilizzata per configurare il comportamento del container NGINX, passiamo in `args` (argomenti di configurazione con cui il processo principale del container deve essere avviato) questo elemento:
```yaml
(...)
args:
	- /nginx-ingress-controller
	- --configmap=$(POD_NAMESPACE)/nginx-configuration
```



#### c) Impostare Variabili d'Ambiente
All'interno del pod, NGINX ha bisogno di conoscere due informazioni fondamentali:

- Il **nome del pod** in cui √® in esecuzione,
- Il **namespace** del cluster in cui √® distribuito.

Questi dati vengono recuperati dinamicamente tramite **variabili d'ambiente** (`POD_NAME` e `POD_NAMESPACE`). Kubernetes ci permette di ottenerli automaticamente utilizzando i **fieldRef**.

#### d) Configurare le Porte per il Traffico
Infine, dobbiamo specificare le **porte** su cui NGINX ricever√† le richieste:

- La **porta 80** per il traffico HTTP,
- La **porta 443** per il traffico HTTPS.

Queste porte sono quelle standard e permetteranno all'Ingress Controller di gestire le connessioni provenienti dall'esterno del cluster.


```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-ingress-controller
spec:
  replicas: 1
  selector:
    matchLabels:
      name: nginx-ingress
  template:
    metadata:
      labels:
        name: nginx-ingress
    spec:
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
          args:
            - /nginx-ingress-controller
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 80
            - name: https
              containerPort: 443
```

Con questo file YAML, abbiamo definito:

1. **Il Deployment**: Kubernetes avvia NGINX con tutte le impostazioni necessarie.
2. **La ConfigMap**: le configurazioni di NGINX sono separate, facili da modificare e gestire.
3. **Le variabili d'ambiente**: dinamicamente recuperate da Kubernetes per adattarsi all'ambiente di esecuzione.
4. **Le porte**: che permettono a NGINX di ascoltare il traffico in ingresso.

Ora NGINX √® pronto per funzionare come Ingress Controller nel nostro cluster Kubernetes.



### Esporre l'ingress controller con un NodePort
Ora abbiamo bisogno di un servizio per esporre l'Ingress controller al modno esterno.
Creiamo quindi un service di ripo **NodePort**, con il selector che corrisponda all'etichetta `name: nginx-ingress` impostata sul deployment 

![[Pasted image 20241217120608.png]]



### Fornire un ServiceAccount all' Ingress Controller
Come accennato in precedenza, gli **Ingress Controller** offrono funzionalit√† avanzate che consentono di monitorare il cluster alla ricerca di risorse **Ingress** e di configurare dinamicamente il server NGINX sottostante ogni volta che vengono rilevate delle modifiche.

Per poter svolgere questa attivit√†, l'Ingress Controller deve avere i **permessi necessari** per interagire con il cluster Kubernetes. Questi permessi vengono garantiti attraverso un **Service Account**, che fornisce al controller l'accesso ai **ruoli** e alle **risorse** del cluster.

In pratica, il Service Account permette all'Ingress Controller di:

1. **Accedere** alle risorse di tipo Ingress presenti nel cluster.
2. **Monitorare** eventuali modifiche a queste risorse.
3. **Configurare** automaticamente il server NGINX per adattarsi ai cambiamenti rilevati.

In assenza di un Service Account adeguato, l'Ingress Controller non avrebbe i privilegi necessari per eseguire queste operazioni fondamentali.

![[Pasted image 20241217134817.png]]



#### Per riassumere
Un **Ingress Controller** richiede i seguenti componenti per funzionare correttamente:

- **Deployment**: utilizza un'immagine NGINX specifica per agire come Ingress Controller.
- **Service**: serve a esporre il controller all'interno del cluster o verso l'esterno.
- **ConfigMap**: fornisce i dati di configurazione necessari per personalizzare il comportamento di NGINX, come percorsi dei log o impostazioni SSL.
- **Service Account**: garantisce i permessi adeguati per monitorare e configurare le risorse Ingress nel cluster.


Dopo l'installazione, l'Ingress Controller monitora costantemente le risorse Ingress definite nel cluster e applica le regole di instradamento richieste, configurando automaticamente il bilanciamento del carico e la gestione delle connessioni sicure, se necessario.





***
## 2. CREARE UN INGRESS RESOURCE
Una volta distribuito l'Ingress Controller, il passaggio successivo √® **creare una risorsa Ingress** (Ingress Resource). Questa risorsa √® un oggetto Kubernetes che definisce un insieme di regole e configurazioni, per permettere all'ingress controller, di instradamento del traffico esterno verso i servizi interni del cluster.

Esempio:
Considerando l'esempio dell'inizio, un ingress resoruce pu√≤ specificare che, a seconda dell'URL inserito, il traffico venga indirizzara ad un applicazione o l'altra

`www.my-online-store.com/wear` --> instrada al portale di vendita vestiti
`www.my-online-store.com/watch` --> instrada al portale di video


### Creare un Ingress Resource
Per creare un Ingress Resource, si utilizza un file YAML in cui si specificano dettagli come i percorsi degli URL, i servizi target e, se necessario, la configurazione SSL per connessioni sicure. 

Ad esempio, puoi definire che le richieste indirizzate a `www.esempio.com/api` vengano inoltrate al servizio interno chiamato `backend-api`, mentre quelle per `www.esempio.com/web` vadano a `frontend-web`.

Il traffico verr√† indirizzato ai services dell'app e non direttamente ai pods, la sezione
**`--> spec.backend`**
definisce dove verr√† instradato il traffico. 
se c'√® un singolo back-end non c'√® bisogno di rules, basta specificare serviceName e port

Un esempio di configurazione YAML per un Ingress Resource potrebbe essere:

![[Pasted image 20241217141638.png]]

Dove:
- La sezione **`spec.backend`** la **destione dove sar√† indirizzato traffico**
- 80 : porta del service `wear-service` che espone il backend

A questo punto l'ingress viene creato e tutto il traffico √® instradato al `wear-service`


### sezione: Rules
La sezione **`rules`** in un file YAML di un oggetto **Ingress** serve a definire le regole che controllano **come il traffico HTTP o HTTPS viene instradato verso i servizi interni del cluster Kubernetes**. √à qui che si specificano i dettagli del traffico, come il nome del dominio (opzionale), i percorsi (URL) e i servizi backend associati.

Possiamo configurare delle **regole** per instradare il traffico in base a determinate condizioni, come:
- il **dominio** o 
- il **nome host** utilizzato dagli utenti.


Ad esempio:
- Possiamo creare una **regola** per gestire il traffico proveniente da un **dominio** specifico. Se gli utenti accedono al cluster utilizzando il dominio `my-online-store.com`, il traffico pu√≤ essere instradato seguendo la **regola 1**.
- Allo stesso modo, se gli utenti accedono utilizzando un sottodominio come `wear.my-online-store.com`, possiamo creare una **regola separata** (ad esempio, la **regola 2**) per instradare e gestire questo traffico in modo distinto.

In questo modo, √® possibile separare il traffico in base al dominio o sottodominio e configurare comportamenti personalizzati per ciascuna regola.

![[Pasted image 20241217144805.png]]


All'interno di ogni **regola** √® possibile configurare percorsi specifici per instradare il traffico verso servizi diversi.

Ad esempio, all'interno della **regola 1**, possiamo gestire:
- il percorso `/wear` per instradare il traffico verso l'applicazione dedicata all'abbigliamento,
- il percorso `/watch` per indirizzare il traffico verso l'applicazione video,
- un percorso predefinito per tutto il traffico restante, garantendo cos√¨ una gestione chiara e organizzata delle diverse applicazioni.

![[Pasted image 20241217145302.png]]


Stessa cosa la posso fare per le altre **rules**. La rule 2 potr√† gestire tutti i percorsi verso applicazione collegare al `www.wear. ...`

![[Pasted image 20241217145508.png]]

In questo caso, digitando `www.wear.my-online-store.com`, l'utente verr√† indirizzato alla pagina principale dello shop. Se invece si digita `wear.my-online-store.com/support`, il traffico verr√† instradato verso backend specifici dedicati al supporto, permettendo cos√¨ di gestire diverse funzionalit√† in base al percorso dell'URL.



Quindi:
![[Pasted image 20241217145800.png]]

- Possiamo avere **rules** per ogni host o nome dominio
- All'interno di ogni regola possiamo avere percorsi per instradare il traffico in base all'URL



# Configurare l'ingress resorce
Quando si parla di routing del traffico in un sistema come Kubernetes, √® possibile farlo utilizzando sia l'[[URL ]](Uniform Resource Locator) che il [[hostname]]. Ecco una spiegazione di entrambi i concetti e delle loro differenze.

## --> Instradare il traffico tramite URL
Il nostro obiettivo √® gestire tutto il traffico in arrivo verso il nostro store online, instradandolo in base al percorso dell'URL. Per questo motivo, abbiamo bisogno di una sola regola, poich√© stiamo gestendo il traffico per un unico dominio, `my-online-store.com`, che ha per√≤ diversi percorsi


![[Pasted image 20241217150148.png]]

Il **routing del traffico per URL** implica che il sistema instradi le richieste in base al **percorso** specificato nell'URL. Questo significa che puoi definire regole di routing che instradano il traffico in base a diverse parti del percorso. Ad esempio:

- Tutte le richieste che iniziano con `/api` potrebbero essere indirizzate a un servizio API.
- Tutte le richieste che iniziano con `/static` potrebbero essere indirizzate a un servizio che gestisce file statici.

Questo tipo di routing √® utile quando si desidera separare diverse funzionalit√† o risorse della stessa applicazione, utilizzando un unico hostname.


All'interno delle regole, troviamo un elemento chiamato `-http[]`, che ci consente di specificare diversi percorsi. Questo significa che i percorsi sono organizzati in un array, con ciascun elemento rappresentante un percorso specifico per un determinato URL.

![[Pasted image 20241217151255.png]]
##### Spiegazione 

**spec**: Questa sezione definisce la configurazione dell'oggetto Ingress.
    - **rules**: Qui si definiscono le regole per il routing del traffico.
        - **http**: Indica che le regole seguono il protocollo HTTP.
            - **paths**: Questa sezione contiene le diverse regole di routing basate sui percorsi.
                - **path: /wear**: Specifica che il traffico in ingresso su `/wear` deve essere indirizzato a un servizio specifico.
                    - **backend**: Definisce il servizio di backend a cui deve essere indirizzato il traffico.
                        - **service**:
                            - **name: wear-service**: Nome del servizio a cui inviare il traffico per questo percorso.
                            - **servicePort: 80**: Porta del servizio a cui inviare il traffico (porta HTTP standard).
                - **path: /watch**: Specifica che il traffico in ingresso su `/watch` deve essere indirizzato a un altro servizio.
                    - **backend**: Definisce il servizio di backend a cui deve essere indirizzato il traffico.
                        - **service**:
                            - **name: watch-service**: Nome del servizio a cui inviare il traffico per questo percorso.
                            - **servicePort: 80**: Porta del servizio a cui inviare il traffico.






## --> Instradare il traffico in base all'hostname - Sezione: Host
Il **routing del traffico per hostname** implica che il sistema instradi le richieste in base al **nome di dominio** specificato nell'URL. Ci√≤ significa che puoi avere regole di routing che separano il traffico in base ai diversi nomi di dominio. Ad esempio:

- Tutto il traffico per `api.example.com` potrebbe essere instradato a un servizio API.
- Tutto il traffico per `www.example.com` potrebbe essere instradato a un servizio web principale.

Questo tipo di routing √® utile per gestire pi√π applicazioni o microservizi che condividono lo stesso cluster, consentendo l'uso di nomi di dominio distinti per ciascuna applicazione.

La sezione **host** in un file YAML di un oggetto Ingress in Kubernetes √® utilizzata per specificare il **nome di dominio (o hostname) a cui si applicano le regole di routing definite nell'Ingress** stesso. Questa sezione consente di instradare il traffico verso servizi diversi in base al nome di dominio utilizzato nella richiesta HTTP.

Ora che ho due domini creer√≤ due **rules**:
![[Pasted image 20241217155758.png]]

Quando si creano due **rules** in un oggetto Ingress per gestire il traffico in base a diversi domini, si utilizza il campo **host** per specificare il nome di dominio associato a ciascuna regola. Questo campo deve corrispondere al valore del nome di dominio utilizzato nell'URL della richiesta. Grazie a questa configurazione, il traffico verr√† instradato verso il backend appropriato in base al dominio richiesto.

** --> NOTA**: Se **non si specifica il campo host**, la regola verr√† considerata come un jolly e accetter√† tutto il traffico, cio√® da tutti gli host 
(`qualsiasi_nome_host_basta_che_finisca_con\quellochespecificoqui`). 
Questo significa che la regola si applicher√† a tutte le richieste, senza limitazioni basate sul dominio utilizzato.

Questa formulazione chiarisce che l'assenza del campo **host** comporta l'accettazione di tutte le richieste e sottolinea che le regole si applicano senza restrizioni basate sul nome del dominio. 



#### Struttura della sezione host
Quando definisci un oggetto Ingress, la sezione host si inserisce all'interno della sezione **rules**. Ecco un esempio di come pu√≤ apparire:

![[Pasted image 20241217160455.png]]


La sezione host in un file YAML di un Ingress √® fondamentale per il routing del traffico HTTP in Kubernetes. Consente di gestire in modo flessibile e sicuro le richieste basate sul nome di dominio, migliorando la scalabilit√† e la manutenibilit√† delle applicazioni nel cluster.



## Differenze Chiave routing per URL e hostname

- **Granularit√†**:
    - **URL**: Permette di suddividere il traffico in base al percorso, consentendo una maggiore granularit√† all'interno dello stesso dominio.
    - **Hostname**: Divide il traffico a livello di dominio, permettendo di gestire servizi distinti sotto nomi di dominio separati.
      
- **Scenari di Utilizzo**:
    - **URL**: Utile quando si desidera organizzare le funzionalit√† di un'applicazione all'interno di un dominio comune.
    - **Hostname**: Utile per applicazioni completamente separate o per implementazioni di microservizi in ambienti multitenant.




***
***
# Aggiornamenti Ingress nella versione Kubernetes 1.20+

### **Principali Modifiche**
1. **apiVersion**:
    
    - **Prima (obsoleta):** `extensions/v1beta1`
        - Questa versione dell'API era in stato **deprecato** e non supportata a partire da **Kubernetes 1.22**.
    - **Ora (attuale):** `networking.k8s.io/v1`
        - La nuova versione √® **stabile** e dovrebbe essere utilizzata per definire gli Ingress.


***

### Backend:
- **Prima (obsoleta)** 
```yaml
backend:
  serviceName: wear-service
  servicePort: 80
```
Qui, il backend specificava i campi `serviceName` e `servicePort` direttamente.

- **attuale**
```yaml
backend:
  service:
    name: wear-service
    port:
      number: 80
```

Il backend ora utilizza una nuova struttura nidificata sotto `service` con:

- **`name`**: Nome del servizio Kubernetes.
- **`port.number`**: Numero della porta del servizio.


***

### PathType
Nella versione **`networking.k8s.io/v1`**, √® obbligatorio specificare il campo **`pathType`**
Il campo **`pathType`** specifica come l'Ingress Controller deve interpretare il valore del percorso definito (`path`). Kubernetes offre tre opzioni: `Exact`, `Prefix` e `ImplementationSpecific`. Ecco la spiegazione dettagliata con esempi pratici.

- **-->`Exact`**: Il percorso deve corrispondere esattamente.
	- Il percorso specificato nella regola deve **corrispondere esattamente** all'URL della richiesta.
	- **Non viene considerato alcun prefisso o estensione** dell'URL oltre al valore fornito.

Esempio:
```yaml
- path: /wear
  pathType: Exact
  backend:
    service:
      name: wear-service
      port:
        number: 80
```
- La richiesta a `http://example.com/wear` viene inviata al servizio **`wear-service`**.
- La richiesta a `http://example.com/wear/extra` o `http://example.com/wear/` viene ignorata perch√© **non corrisponde esattamente**.




- **-->`Prefix`**: Il percorso √® trattato come prefisso.
	- Il percorso viene interpretato come un **prefisso**.
	- Qualsiasi URL che **inizia** con il percorso specificato corrisponder√† alla regola.

Esempio:
```yaml
- path: /wear
  pathType: Prefix
  backend:
    service:
      name: wear-service
      port:
        number: 80

```

-  La richiesta a `http://example.com/wear` corrisponde.
- La richiesta a `http://example.com/wear/shoes` corrisponde perch√© **inizia** con `/wear`.
- La richiesta a `http://example.com/watch` **non** corrisponde perch√© non ha il prefisso `/wear`.

üîπ **Nota:** Questo √® il comportamento pi√π comunemente utilizzato e supportato da tutti gli Ingress Controller.





- **-->`ImplementationSpecific`**: 
	-  Il comportamento del percorso dipende dall'**implementazione specifica** dell'Ingress Controller utilizzato (es. NGINX, Traefik, HAProxy).
	- Ogni Ingress Controller pu√≤ definire come gestire i percorsi, il che significa che il comportamento **non √® standardizzato**.


**Esempio**:
```yaml
- path: /wear
  pathType: ImplementationSpecific
  backend:
    service:
      name: wear-service
      port:
        number: 80
```

- Il comportamento del percorso dipende dal controller:
    - In NGINX, potrebbe comportarsi come `Prefix` per default.
    - In un altro controller, potrebbe comportarsi diversamente.
- **Devi controllare la documentazione** del controller per sapere come verr√† interpretato il percorso.



***

### **Riepilogo delle Differenze**

|**Campo**|**Versione Obsoleta (v1beta1)**|**Versione Aggiornata (v1)**|
|---|---|---|
|**API Version**|`extensions/v1beta1`|`networking.k8s.io/v1`|
|**Service Backend**|`serviceName` e `servicePort`|`service.name` e `service.port.number`|
|**PathType**|Non presente|Obbligatorio (`Prefix`, `Exact`, `ImplementationSpecific`)|


### Esempio Aggiornato Completo
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
spec:
  rules:
    - http:
        paths:
        - path: /wear
          pathType: Prefix
          backend:
            service:
              name: wear-service
              port:
                number: 80
        - path: /watch
          pathType: Prefix
          backend:
            service:
              name: watch-service
              port:
                number: 80
```









***
***
## COMANDI 

**Creare un Ingress Resource**
``` bash
kubectl create -f ingress.yaml
```

**Visualizzare gli ingress Resorces (non il deployment ingress contoll)**
```bash
kubectl get ingress
```

**Ottenere dettagli su un Ingress specifico**
```bash
kubectl describe ingress <nome-ingress>
```
##### Informazioni Comuni nell'Output
1. **Name**: Il nome dell'oggetto Ingress.
    
2. **Namespace**: Il namespace in cui si trova l'Ingress.
    
3. **Annotations**: Eventuali annotazioni associate all'Ingress, che possono fornire informazioni aggiuntive o configurazioni specifiche per l'Ingress Controller.
    
4. **Class**: La classe dell'Ingress, se √® stata specificata. Questo determina quale Ingress Controller gestisce questa risorsa.
    
5. **Rules**: Le regole di routing definite per l'Ingress, inclusi i percorsi e i servizi backend associati:
    - **Host**: Il nome dell'host per cui √® valida la regola (se specificato).
    - **Path**: I percorsi definiti (come `/wear` e `/watch`).
    - **Service**: Il nome del servizio di backend e la porta a cui inviare il traffico.

6. **Address**: L'indirizzo IP o il nome del DNS dell'Ingress, se √® stato esposto e registrato correttamente.
    
7. **Events**: Eventuali eventi associati all'Ingress, che possono indicare stati di errore o altre attivit√† recenti relative all'Ingress.



#### **IMPERATIVE: Creare Ingress resource**
```bash
kubectl create ingress <ingress-name> --rule="host/path=service:port"
```
Crea un oggetto Ingress con una regola specificata.

Esempio:
```bash
kubectl create ingress <ingress-name> --rule="wear.my-online-store.com/wear*=wear-service:80"
```
Instrada il traffico per **`wear.my-online-store.com`** con percorso `/wear*` verso il servizio **`wear-service`** sulla porta **80**.



#### **Creare un ingress in un namespace**
**1. Vedere quale service √® presente nel namespace**
```bash
k get svc -n <nome-namespace>
```

Da qui andr√≤ a vedere:
- `NAME`: Il nome del service (es: pay-service)
- `PORT`: La porta su cui √® esposto il servizio (es: 8282)

**2. Imperative command**
```bash
k create ingress <nome-ingress> -n <nome-namespace> --rule="/pay=<nome-servizio>:<porta-del-servizio"
```
- `/pay` : tuttp il traffico indirizzato a /pay
- `=` : andr√†
- `<nome-servizio>`: a questo servizio
- `<porta-servizio>`: su questa porta